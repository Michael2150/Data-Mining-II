{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68752ef6b533e068",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 04 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9baea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = 0.4 #% of the data that is used for training to speed up the process of finding the best model\n",
    "should_build_model = False #If False, the model will be loaded from a file.\n",
    "should_save_model = True #If True, the model will be saved to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e132f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from pprint import pprint \n",
    "\n",
    "DEBUG = True\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14577343c1f65c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET = \"df_processed.pkl\"\n",
    "SCORE_DATASET = \"df_score_processed.pkl\"\n",
    "\n",
    "import os, sys\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "ROOT = \"./\"\n",
    "\n",
    "if COLAB:\n",
    "  from google.colab import drive\n",
    "  if not os.path.isdir(\"/content/gdrive\"):\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    d = \"/content/gdrive/MyDrive/datasets\"\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  ROOT = f\"/content/gdrive/MyDrive/datasets/{DATASET.replace(' ','_')}/\"\n",
    "  if not os.path.isdir(ROOT): os.makedirs(ROOT)\n",
    "\n",
    "\n",
    "def makedirs(d):\n",
    "  if COLAB:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  else:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d, mode=0o777, exist_ok=True)\n",
    "\n",
    "for d in ['orig','data','output']: makedirs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa6c75",
   "metadata": {},
   "source": [
    "## Import Data & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f69e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(ROOT\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mDATASET)\n\u001b[0;32m      2\u001b[0m df_score \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(ROOT\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mSCORE_DATASET)\n\u001b[0;32m      4\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df, df_score]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(ROOT+\"data/\"+DATASET)\n",
    "df_score = pd.read_pickle(ROOT+\"data/\"+SCORE_DATASET)\n",
    "\n",
    "dfs = [df, df_score]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47025d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ROOT+\"data/features.yaml\") as file:\n",
    "    yml_obj = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "target = yml_obj[\"target\"]\n",
    "features = yml_obj[\"features\"]\n",
    "numerical_features = yml_obj[\"numerical_features\"]\n",
    "categorical_features = yml_obj[\"categorical_features\"]\n",
    "\n",
    "target_labels = sorted(df[target].unique())\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Numerical Features: {numerical_features}\")\n",
    "print(f\"Categorical Features: {categorical_features}\")\n",
    "print(f\"Target Labels: {target_labels}\")\n",
    "print(f\"Passed the sanity check: {len(numerical_features) + len(categorical_features) == len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400d442",
   "metadata": {},
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50fabe",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183378af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c04a21",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b71c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(y_pred_prob, name=f\"submission-{pd.to_datetime('now', utc=True).strftime('%Y%m%d%H%M%S')}\"):\n",
    "    df_sub = pd.DataFrame(y_pred_prob, columns=target_labels)\n",
    "    df_sub.index.name = 'id'\n",
    "    df_sub.to_csv(ROOT+f'output/{name}.csv', index=True)\n",
    "    print(f\"Saved ({df_sub.shape[0]} rows) to: {ROOT}output/{name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea62ab5",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "Data is being split into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate the model.\n",
    "\n",
    "The following dataframes are created:\n",
    "- `X_train`: Features of the training set\n",
    "- `X_test`: Features of the testing set\n",
    "- `y_train`: Target of the training set\n",
    "- `y_test`: Target of the testing set\n",
    "\n",
    "Stratify is used to ensure that the target distribution is the same in both the training and testing sets. (We saw a small imbalance in the target distribution in the EDA notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train , df_test = train_test_split(df, test_size=1-train_dataset_size, random_state=SEED, stratify=df[target])\n",
    "\n",
    "df_train.sort_index(inplace=True)\n",
    "df_test.sort_index(inplace=True)\n",
    "\n",
    "print(df_train.shape, df_test.shape, df_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef995a1",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "The features are scaled using the `StandardScaler` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df_train[numerical_features])\n",
    "\n",
    "x_train_num = ss.transform(df_train[numerical_features])\n",
    "x_test_num = ss.transform(df_test[numerical_features])\n",
    "x_score_num = ss.transform(df_score[numerical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6564b20",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "- The target feature is encoded using the `LabelEncoder` from `sklearn.preprocessing`\n",
    "- The categorical features are encoded using the `OneHotEncoder` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df[target])\n",
    "\n",
    "y_train = le.transform(df_train[target])\n",
    "y_test = le.transform(df_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(df_train[categorical_features])\n",
    "\n",
    "x_train_cat = ohe.transform(df_train[categorical_features])\n",
    "x_test_cat = ohe.transform(df_test[categorical_features])\n",
    "x_score_cat = ohe.transform(df_score[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c73652",
   "metadata": {},
   "source": [
    "### Merge Encoded and Scaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "column_names = numerical_features + list(ohe_feature_names)\n",
    "\n",
    "x_train = pd.DataFrame(np.concatenate([x_train_num, x_train_cat.toarray()], axis=1), columns=column_names)\n",
    "x_test = pd.DataFrame(np.concatenate([x_test_num, x_test_cat.toarray()], axis=1), columns=column_names)\n",
    "x_score = pd.DataFrame(np.concatenate([x_score_num, x_score_cat.toarray()], axis=1), columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, x_test.shape, x_score.shape)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95b135",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2879c",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "May require XGBoost to be installed. Uncomment the following line to install it.\n",
    "I used the following command to install XGBoost:\n",
    "```bash\n",
    "conda install xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# pickle\n",
    "import pickle\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be896f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(name, classifier, x_train, y_train):\n",
    "    if should_build_model:\n",
    "        print(f\"Training {name}...\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        classifier.fit(x_train, y_train)\n",
    "        end_time = pd.Timestamp.now()\n",
    "        print(f\"Training {name} took: {end_time - start_time}\")\n",
    "\n",
    "        if should_save_model:\n",
    "            with open(ROOT+f\"output/{name}.pkl\", 'wb') as file:\n",
    "                pickle.dump(classifier, file)\n",
    "                print(f\"Saved model to: {ROOT}output/{name}.pkl\")\n",
    "                \n",
    "    else:\n",
    "        with open(ROOT+f\"output/{name}.pkl\", 'rb') as file:\n",
    "            classifier = pickle.load(file)\n",
    "            print(f\"Loaded model from: `{ROOT}output/{name}.pkl`\")\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, classifier, x_test, y_test):\n",
    "    # evaluate model\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    y_pred_prob = classifier.predict_proba(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='weighted')\n",
    "\n",
    "    print(f\"Evaluation for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"Logistic_Regression\"\n",
    "classifiers.update({\n",
    "    classifier_name : LogisticRegression(random_state=SEED, max_iter=1000, n_jobs=-1),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"Random_Forest_(max_depth=10)\"\n",
    "classifiers.update({\n",
    "    classifier_name : RandomForestClassifier(random_state=SEED, n_jobs=-1, max_depth=10),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41797b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOOK TOO LONG TO TRAIN 14+ minutes\n",
    "\n",
    "# classifier_name = \"Gradient Boosting\"\n",
    "# classifiers.update({\n",
    "#     classifier_name : GradientBoostingClassifier(random_state=SEED),\n",
    "# })\n",
    "\n",
    "# classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TOOK TOO LONG TO TRAIN 18+ minutes\n",
    "\n",
    "# classifier_name = \"Support Vector Machine\"\n",
    "# classifiers.update({\n",
    "#     classifier_name : SVC(probability=True, random_state=SEED),\n",
    "# })\n",
    "\n",
    "# classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"Neural_Network\"\n",
    "classifiers.update({\n",
    "    classifier_name : MLPClassifier(random_state=SEED),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ce4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"Naive_Bayes\"\n",
    "classifiers.update({\n",
    "    classifier_name : GaussianNB(),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"K-Nearest_Neighbors_(3)\"\n",
    "classifiers.update({\n",
    "    classifier_name : KNeighborsClassifier(n_jobs=-1, n_neighbors=3),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"K-Nearest_Neighbors_(5)\"\n",
    "classifiers.update({\n",
    "    classifier_name : KNeighborsClassifier(n_jobs=-1, n_neighbors=5),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"K-Nearest_Neighbors_(7)\"\n",
    "classifiers.update({\n",
    "    classifier_name : KNeighborsClassifier(n_jobs=-1, n_neighbors=7),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"XGBoost\"\n",
    "classifiers.update({\n",
    "    classifier_name : XGBClassifier(random_state=SEED),\n",
    "})\n",
    "\n",
    "classifiers[classifier_name] = setup_model(classifier_name, classifiers[classifier_name], x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4f2fd",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier_name, classifier in classifiers.items():\n",
    "    evaluate_model(classifier_name, classifier, x_test, y_test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281e15a",
   "metadata": {},
   "source": [
    "## Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242413df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86bf24a3",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
