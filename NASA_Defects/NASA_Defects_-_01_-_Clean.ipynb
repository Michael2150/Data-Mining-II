{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Defects - 01 - Import & Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import yaml\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from pprint import pprint \n",
    "\n",
    "DEBUG = True\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"jm1.csv\"\n",
    "\n",
    "import os, sys\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "ROOT = \"./\"\n",
    "\n",
    "if COLAB:\n",
    "  from google.colab import drive\n",
    "  if not os.path.isdir(\"/content/gdrive\"):\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    d = \"/content/gdrive/MyDrive/datasets\"\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  ROOT = f\"/content/gdrive/MyDrive/datasets/{DATASET.replace(' ','_')}/\"\n",
    "  if not os.path.isdir(ROOT): os.makedirs(ROOT)\n",
    "\n",
    "\n",
    "def makedirs(d):\n",
    "  if COLAB:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  else:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d, mode=0o777, exist_ok=True)\n",
    "\n",
    "for d in ['orig','data','output']: makedirs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10878, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC_BLANK</th>\n",
       "      <th>BRANCH_COUNT</th>\n",
       "      <th>LOC_CODE_AND_COMMENT</th>\n",
       "      <th>LOC_COMMENTS</th>\n",
       "      <th>CYCLOMATIC_COMPLEXITY</th>\n",
       "      <th>DESIGN_COMPLEXITY</th>\n",
       "      <th>ESSENTIAL_COMPLEXITY</th>\n",
       "      <th>LOC_EXECUTABLE</th>\n",
       "      <th>HALSTEAD_CONTENT</th>\n",
       "      <th>HALSTEAD_DIFFICULTY</th>\n",
       "      <th>HALSTEAD_EFFORT</th>\n",
       "      <th>HALSTEAD_ERROR_EST</th>\n",
       "      <th>HALSTEAD_LENGTH</th>\n",
       "      <th>HALSTEAD_LEVEL</th>\n",
       "      <th>HALSTEAD_PROG_TIME</th>\n",
       "      <th>HALSTEAD_VOLUME</th>\n",
       "      <th>NUM_OPERANDS</th>\n",
       "      <th>NUM_OPERATORS</th>\n",
       "      <th>NUM_UNIQUE_OPERANDS</th>\n",
       "      <th>NUM_UNIQUE_OPERATORS</th>\n",
       "      <th>LOC_TOTAL</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>447.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>2824.0</td>\n",
       "      <td>210.28</td>\n",
       "      <td>384.45</td>\n",
       "      <td>31079782.27</td>\n",
       "      <td>26.95</td>\n",
       "      <td>8441.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1726654.57</td>\n",
       "      <td>80843.08</td>\n",
       "      <td>3021.0</td>\n",
       "      <td>5420.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>3442.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1588.0</td>\n",
       "      <td>202.98</td>\n",
       "      <td>213.53</td>\n",
       "      <td>9254819.86</td>\n",
       "      <td>14.45</td>\n",
       "      <td>4828.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>514156.64</td>\n",
       "      <td>43342.31</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>3172.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>108.14</td>\n",
       "      <td>46.32</td>\n",
       "      <td>232043.52</td>\n",
       "      <td>1.67</td>\n",
       "      <td>685.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>12891.31</td>\n",
       "      <td>5009.32</td>\n",
       "      <td>295.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>101.20</td>\n",
       "      <td>206.01</td>\n",
       "      <td>4294926.45</td>\n",
       "      <td>6.95</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>238607.05</td>\n",
       "      <td>20848.47</td>\n",
       "      <td>813.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOC_BLANK  BRANCH_COUNT  LOC_CODE_AND_COMMENT  LOC_COMMENTS  CYCLOMATIC_COMPLEXITY  DESIGN_COMPLEXITY  ESSENTIAL_COMPLEXITY  LOC_EXECUTABLE  HALSTEAD_CONTENT  HALSTEAD_DIFFICULTY  HALSTEAD_EFFORT  HALSTEAD_ERROR_EST  HALSTEAD_LENGTH  HALSTEAD_LEVEL  HALSTEAD_PROG_TIME  HALSTEAD_VOLUME  NUM_OPERANDS  NUM_OPERATORS  NUM_UNIQUE_OPERANDS  NUM_UNIQUE_OPERATORS  LOC_TOTAL  label\n",
       "0      447.0         826.0                  12.0         157.0                  470.0              385.0                 113.0          2824.0            210.28               384.45      31079782.27               26.95           8441.0            0.00          1726654.57         80843.08        3021.0         5420.0                609.0                 155.0     3442.0      1\n",
       "1        0.0         211.0                   0.0           0.0                  128.0              104.0                  14.0             0.0              0.00                 0.00             0.00                0.00              0.0            0.00                0.00             0.00           0.0            0.0                  0.0                   0.0     1129.0      1\n",
       "2      164.0         485.0                  10.0          58.0                  268.0              219.0                  39.0          1588.0            202.98               213.53       9254819.86               14.45           4828.0            0.00           514156.64         43342.31        1730.0         3172.0                407.0                 102.0     1824.0      1\n",
       "3       37.0          29.0                   8.0          42.0                   19.0               19.0                   6.0           133.0            108.14                46.32        232043.52                1.67            685.0            0.02            12891.31          5009.32         295.0          390.0                121.0                  38.0      222.0      1\n",
       "4       11.0         405.0                   0.0          17.0                  404.0                2.0                   1.0           814.0            101.20               206.01       4294926.45                6.95           2033.0            0.00           238607.05         20848.47         813.0         1220.0                811.0                 411.0      844.0      1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ROOT+\"orig/\"+DATASET)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Value Checks\n",
    "\n",
    "Here we can see that there are no apparent null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the dataset:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Null values in the dataset: \", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Cases & Features\n",
    "\n",
    "We are expected to see 10878 cases and 24 features. But looking at the dataset I've found that there are 10878 cases, which is correct, but there are 21 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 'label'\n",
      "Cases: 10878\n",
      "Features: 21\n"
     ]
    }
   ],
   "source": [
    "target = \"label\"\n",
    "features = list(df.columns)\n",
    "features.remove(target)\n",
    "counts_features = [col for col in df.columns if \"COUNT\" in col or \"LOC\" in col or \"NUM\" in col]\n",
    "\n",
    "print(f\"Target: '{target}'\")\n",
    "print(f\"Cases: {len(df)}\")\n",
    "print(f\"Features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implausible Value Checks\n",
    "\n",
    "Next I need to check if the dataset contains any implausible values. The document states the following:\n",
    "1. `LOC_TOTAL` is never = 0\n",
    "2. values are always positive ie >=0\n",
    "3. count values are always integers\n",
    "\n",
    "Results:\n",
    "Based on the results from the following 3 checks, I can see that there are no implausible values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) `LOC_TOTAL` is never = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1 is True\n"
     ]
    }
   ],
   "source": [
    "# Check that `LOC_TOTAL` is always positive and non-zero\n",
    "check_1 = df[df[\"LOC_TOTAL\"] <= 0]\n",
    "print(f\"Check 1 is {check_1.shape[0] == 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Values are always positive ie >=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 2 is True\n"
     ]
    }
   ],
   "source": [
    "# Check that all columns are positive or zero\n",
    "check_2 = True\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [np.float64, np.int64]:\n",
    "        filter = df[col] < 0\n",
    "        check_2 = check_2 and (len(df[filter]) == 0)\n",
    "print(f\"Check 2 is {check_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Count values are always integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 3 is True\n"
     ]
    }
   ],
   "source": [
    "# Check that all count features are integers\n",
    "check_3 = True\n",
    "for col in counts_features:\n",
    "    filter = (df[col] % 1 != 0)\n",
    "    check_3 = check_3 and (len(df[filter]) == 0)\n",
    "print(f\"Check 3 is {check_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Def is_implausible\n",
    "This def checks for a given value and column, if it does not satisfy the above 3 conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a value is implausible for a given feature\n",
    "def is_implausible(case_value, feature_name):\n",
    "    if feature_name == \"LOC_TOTAL\":\n",
    "        return case_value <= 0\n",
    "    if feature_name in counts_features:\n",
    "        return case_value % 1 != 0 or case_value < 0\n",
    "    return case_value < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conflicting Values Checks\n",
    "\n",
    "This def returns a list of filters that can be used to filter the dataset to find conflicting values.\n",
    "\n",
    "It got a little out of hand because I wanted to print each filter when I tested tweaking the atol and rtol values.\n",
    "The next cell will do the printing of the values I stuck with.\n",
    "\n",
    "10. `HALSTEAD_LENGTH` = `NUM_OPERATORS` + `NUM_OPERANDS` \n",
    "    \n",
    "13) `HALSTEAD_VOLUME` = (`NUM_OPERATORS` + `NUM_OPERANDS`) * log2(`NUM_UNIQUE_OPERATORS` + `NUM_UNIQUE_OPERANDS`)\n",
    "14) `HALSTEAD_LEVEL` = (2 / `NUM_UNIQUE_OPERATORS`) * (`NUM_UNIQUE_OPERANDS` / `NUM_OPERANDS`)\n",
    "15)  `HALSTEAD_DIFFICULTY` = (`NUM_UNIQUE_OPERATORS` / 2) * (`NUM_OPERANDS` / `NUM_UNIQUE_OPERANDS`)\n",
    "16)  `HALSTEAD_CONTENT` = `HALSTEAD_VOLUME` / `HALSTEAD_DIFFICULTY`\n",
    "17)  `HALSTEAD_EFFORT` = `HALSTEAD_VOLUME` * `HALSTEAD_DIFFICULTY`\n",
    "18)  `HALSTEAD_PROG_TIME` = `HALSTEAD_EFFORT` / 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrity_check_filters(df):\n",
    "    \n",
    "    # `HALSTEAD_LENGTH` = `NUM_OPERATORS` + `NUM_OPERANDS` \n",
    "    filter_10 = {\n",
    "        \"name\" : \"(10) HALSTEAD_LENGTH\",\n",
    "        \"a\" : df[\"HALSTEAD_LENGTH\"],\n",
    "        \"b\" : df[\"NUM_OPERATORS\"] + df[\"NUM_OPERANDS\"],\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"NUM_OPERATORS\", \"NUM_OPERANDS\", \"HALSTEAD_LENGTH\"]\n",
    "    }\n",
    "\n",
    "    # Checks for 0 values when doing the log calculation for check 13\n",
    "    calculated_log_paramater = df.apply( lambda row:\n",
    "        np.log2(row[\"NUM_UNIQUE_OPERATORS\"] + row[\"NUM_UNIQUE_OPERANDS\"]) \n",
    "        if row[\"NUM_UNIQUE_OPERATORS\"] + row[\"NUM_UNIQUE_OPERANDS\"] != 0 \n",
    "        else 0\n",
    "    , axis=1)\n",
    "\n",
    "    # `HALSTEAD_VOLUME` = (`NUM_OPERATORS` + `NUM_OPERANDS`) * log2(`NUM_UNIQUE_OPERATORS` + `NUM_UNIQUE_OPERANDS`)\n",
    "    filter_13 = {\n",
    "        \"name\" : \"(13) HALSTEAD_VOLUME\",\n",
    "        \"a\" : df[\"HALSTEAD_VOLUME\"],\n",
    "        \"b\" : (df[\"NUM_OPERATORS\"] + df[\"NUM_OPERANDS\"]) * calculated_log_paramater,\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.001,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"NUM_OPERATORS\", \"NUM_OPERANDS\", \"NUM_UNIQUE_OPERATORS\", \"NUM_UNIQUE_OPERANDS\", \"HALSTEAD_VOLUME\"]\n",
    "    }\n",
    "\n",
    "    # Checks for 0 values when doing the division for check 14\n",
    "    calculated_division_parameter = df.apply( lambda row:\n",
    "        (2 / row[\"NUM_UNIQUE_OPERATORS\"]) * (row[\"NUM_UNIQUE_OPERANDS\"] / row[\"NUM_OPERANDS\"])\n",
    "        if row[\"NUM_UNIQUE_OPERATORS\"] != 0 and row[\"NUM_OPERANDS\"] != 0\n",
    "        else 0\n",
    "    , axis=1)\n",
    "\n",
    "    # `HALSTEAD_LEVEL` = (2 / `NUM_UNIQUE_OPERATORS`) * (`NUM_UNIQUE_OPERANDS` / `NUM_OPERANDS`)\n",
    "    filter_14 = {\n",
    "        \"name\" : \"(14) HALSTEAD_LEVEL\",\n",
    "        \"a\" : df[\"HALSTEAD_LEVEL\"],\n",
    "        \"b\" : calculated_division_parameter,\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.1,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"NUM_UNIQUE_OPERATORS\", \"NUM_OPERANDS\", \"NUM_UNIQUE_OPERANDS\", \"HALSTEAD_LEVEL\"]\n",
    "    }\n",
    "\n",
    "    # Checks for 0 values when doing the division for check 15\n",
    "    calculated_division_parameter_2 = df.apply( lambda row:\n",
    "        (row[\"NUM_OPERANDS\"] / row[\"NUM_UNIQUE_OPERANDS\"])\n",
    "        if row[\"NUM_UNIQUE_OPERANDS\"] != 0\n",
    "        else 0\n",
    "    , axis=1)\n",
    "\n",
    "    # `HALSTEAD_DIFFICULTY` = (`NUM_UNIQUE_OPERATORS` / 2) * (`NUM_OPERANDS` / `NUM_UNIQUE_OPERANDS`)\n",
    "    filter_15 = {\n",
    "        \"name\" : \"(15) HALSTEAD_DIFFIC\",\n",
    "        \"a\" : df[\"HALSTEAD_DIFFICULTY\"],\n",
    "        \"b\" : ((df[\"NUM_UNIQUE_OPERATORS\"] / 2) * calculated_division_parameter_2),\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.001,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"NUM_OPERANDS\", \"NUM_UNIQUE_OPERANDS\", \"NUM_UNIQUE_OPERATORS\", \"HALSTEAD_DIFFICULTY\"]\n",
    "    }\n",
    "\n",
    "    # checks for 0 values when doing the division for check 16\n",
    "    calculated_division_parameter_3 = df.apply( lambda row:\n",
    "        (row[\"HALSTEAD_VOLUME\"] / row[\"HALSTEAD_DIFFICULTY\"])\n",
    "        if row[\"HALSTEAD_DIFFICULTY\"] != 0\n",
    "        else 0\n",
    "    , axis=1)\n",
    "\n",
    "    # `HALSTEAD_CONTENT` = `HALSTEAD_VOLUME` / `HALSTEAD_DIFFICULTY`\n",
    "    filter_16 = {\n",
    "        \"name\" : \"(16) HALSTEAD_CONTENT\",\n",
    "        \"a\" : df[\"HALSTEAD_CONTENT\"],\n",
    "        \"b\" : calculated_division_parameter_3,\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.001,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"HALSTEAD_VOLUME\", \"HALSTEAD_DIFFICULTY\", \"HALSTEAD_CONTENT\"]\n",
    "    }\n",
    "\n",
    "    # `HALSTEAD_EFFORT` = `HALSTEAD_VOLUME` * `HALSTEAD_DIFFICULTY`\n",
    "    filter_17 = {\n",
    "        \"name\" : \"(17) HALSTEAD_EFFORT\",\n",
    "        \"a\" : df[\"HALSTEAD_EFFORT\"],\n",
    "        \"b\" : (df[\"HALSTEAD_VOLUME\"] * df[\"HALSTEAD_DIFFICULTY\"]),\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.01,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"HALSTEAD_VOLUME\", \"HALSTEAD_DIFFICULTY\", \"HALSTEAD_EFFORT\"]\n",
    "    }\n",
    "\n",
    "    # `HALSTEAD_PROG_TIME` = `HALSTEAD_EFFORT` / 18\n",
    "    filter_18 = {\n",
    "        \"name\" : \"(18) HALSTEAD_PROG_TIME\",\n",
    "        \"a\" : df[\"HALSTEAD_PROG_TIME\"],\n",
    "        \"b\" : (df[\"HALSTEAD_EFFORT\"] / 18),\n",
    "        \"atol\" : 0,\n",
    "        \"rtol\" : 0.01,\n",
    "        \"filter\" : None,\n",
    "        \"cols_involved\" : [\"HALSTEAD_EFFORT\", \"HALSTEAD_PROG_TIME\"]\n",
    "    }\n",
    "\n",
    "    filters = [filter_10, filter_13, filter_14, filter_15, filter_16, filter_17, filter_18] \n",
    "\n",
    "    for filter in filters:\n",
    "        filter[\"filter\"] = np.isclose(filter[\"a\"], filter[\"b\"], atol=filter[\"atol\"], rtol=filter[\"rtol\"])\n",
    "\n",
    "    return filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the above filters\n",
    "\n",
    "This is just a test to see what the filters do when applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10) HALSTEAD_LENGTH\n",
      "filter:\t139 rows (139 new)\n",
      "atol:\t0\trtol:\t0\n",
      "\n",
      "(13) HALSTEAD_VOLUME\n",
      "filter:\t147 rows (10 new)\n",
      "atol:\t0\trtol:\t0.001\n",
      "\n",
      "(14) HALSTEAD_LEVEL\n",
      "filter:\t933 rows (869 new)\n",
      "atol:\t0\trtol:\t0.1\n",
      "\n",
      "(15) HALSTEAD_DIFFIC\n",
      "filter:\t253 rows (191 new)\n",
      "atol:\t0\trtol:\t0.001\n",
      "\n",
      "(16) HALSTEAD_CONTENT\n",
      "filter:\t211 rows (53 new)\n",
      "atol:\t0\trtol:\t0.001\n",
      "\n",
      "(17) HALSTEAD_EFFORT\n",
      "filter:\t0 rows (0 new)\n",
      "atol:\t0\trtol:\t0.01\n",
      "\n",
      "(18) HALSTEAD_PROG_TIME\n",
      "filter:\t0 rows (0 new)\n",
      "atol:\t0\trtol:\t0.01\n",
      "\n",
      "Total rows with integrity check failures: 1262\n"
     ]
    }
   ],
   "source": [
    "def assess_filter(df, filter, failure_rows):\n",
    "    filter_df = df[~filter]\n",
    "    filter_df_row_count = len(filter_df)\n",
    "    new_filter_rows = 0\n",
    "    for index, row in filter_df.iterrows():\n",
    "        if index not in failure_rows:\n",
    "            new_filter_rows += 1\n",
    "            failure_rows.add(index)\n",
    "    return filter_df_row_count, new_filter_rows\n",
    "\n",
    "def TestFilters(df):\n",
    "    filter_objs = get_integrity_check_filters(df)\n",
    "\n",
    "    failure_rows = set()\n",
    "\n",
    "    for filter_obj in filter_objs:\n",
    "        filter_df_row_count, new_filter_rows = assess_filter(df, filter_obj[\"filter\"], failure_rows)\n",
    "\n",
    "        print(f\"{filter_obj['name']}\\n\"\n",
    "        f\"filter:\\t{filter_df_row_count} rows ({new_filter_rows} new)\\n\"\n",
    "        f\"atol:\\t{filter_obj['atol']}\\trtol:\\t{filter_obj['rtol']}\\n\"\n",
    "        )\n",
    "\n",
    "    print(f\"Total rows with integrity check failures: {len(failure_rows)}\")\n",
    "\n",
    "TestFilters(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data quality\n",
    "\n",
    "### A - Identical features\n",
    "\n",
    "Refers to a situation where two or more features contain identical values for all cases.\n",
    "\n",
    "F1=F2=F3 ∧ F4=F5 =⇒ 3 features are identical so could be deleted.\n",
    "\n",
    "Expected output: 0\n",
    " \n",
    "Results: 0 - There are no identical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical columns: 0\n"
     ]
    }
   ],
   "source": [
    "checkA = 0\n",
    "\n",
    "cols = df.columns\n",
    "for i in range(len(cols)-2): # Go through colums 0 to n-2\n",
    "    for j in range(i+1, len(cols)-1): # Go through columns 1 to n-1\n",
    "        for k in range(j+1, len(cols)): # Go through columns 2 to n\n",
    "            if df[cols[i]].equals(df[cols[j]]) and df[cols[j]].equals(df[cols[k]]):\n",
    "                print(f\"Columns {cols[i]}, {cols[j]} and {cols[k]} are identical\")\n",
    "                checkA += 1\n",
    "\n",
    "print(f\"Identical columns: {checkA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B - Constant features\n",
    "\n",
    "Refers to features that contain the same value for every instance, i.e. add no information\n",
    "\n",
    "Expected output: 0\n",
    "\n",
    "Results: 0 - There are no constant features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with the same value for all rows: 0\n"
     ]
    }
   ],
   "source": [
    "checkB = 0\n",
    "\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        print(f\"Column {col} has the same value for all rows\")\n",
    "        checkB += 1\n",
    "\n",
    "print(f\"Columns with the same value for all rows: {checkB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C - Features with missing values\n",
    "\n",
    "Counts the number of features that contain one or more missing observations\n",
    "\n",
    "F1 has 10 missing values ∧ F3 has 3 missing values =⇒ 2 features contain missing values.\n",
    "\n",
    "Expected output: 0\n",
    "\n",
    "Results: 0 - There are no features with missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values: 0\n"
     ]
    }
   ],
   "source": [
    "checkC = df.isnull().sum().sum()\n",
    "for col in df.columns:\n",
    "    missing_vals = df[col].isnull().sum()\n",
    "    if missing_vals > 0:\n",
    "        print(f\"Column {col} has {missing_vals} missing values\")\n",
    "\n",
    "print(f\"Total missing values: {checkC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D - Features with conflicting values\n",
    "\n",
    "Counts features that violate some referential integrity constraint\n",
    "\n",
    "F1 should equal F2+F3 but does not. We cannot say which feature is in error therefore =⇒ 3 problematic features.\n",
    "\n",
    "Expected output: 9\n",
    "\n",
    "Results: 11 - There are 11 features with conflicting values in the dataset according to the checks I've made. The checks I have are not the same as the ones in the document, but there are no way of knowing the methods used in the paper to check for conflicting values. This will pop up again in the check J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflicting features (x11): {'NUM_OPERATORS', 'HALSTEAD_VOLUME', 'HALSTEAD_LENGTH', 'HALSTEAD_LEVEL', 'HALSTEAD_PROG_TIME', 'HALSTEAD_DIFFICULTY', 'HALSTEAD_CONTENT', 'NUM_UNIQUE_OPERANDS', 'HALSTEAD_EFFORT', 'NUM_OPERANDS', 'NUM_UNIQUE_OPERATORS'}\n"
     ]
    }
   ],
   "source": [
    "conflicting_value_filters = get_integrity_check_filters(df)\n",
    "conflicting_features = set()\n",
    "\n",
    "for filter in conflicting_value_filters:\n",
    "    filter_df = df[~filter[\"filter\"]]\n",
    "    conflicting_features.update(filter[\"cols_involved\"])\n",
    "\n",
    "checkD = len(conflicting_features)\n",
    "\n",
    "print(f\"Conflicting features (x{checkD}): {conflicting_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E - Features with implausible values\n",
    "\n",
    "Counts features that violate some integrity constraint\n",
    "\n",
    "F1 should be non-negative but contains 1 or more instances < 0 =⇒ 1 problematic feature\n",
    "\n",
    "Expected output: 0\n",
    "\n",
    "Results: 0 - There are no features with implausible values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implausible values count: 0\n"
     ]
    }
   ],
   "source": [
    "implausible_values = set()\n",
    "for col in df.columns:\n",
    "    for value in df[col]:\n",
    "        if is_implausible(value, col):\n",
    "            implausible_values.add((col, value))\n",
    "\n",
    "checkE = len(implausible_values)\n",
    "\n",
    "print(f\"Implausible values count: {checkE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F - Total problem features\n",
    "\n",
    "Count of features impacted by 1 or more of A-E. Since features may contain more than one problem this need not be the sum of A to E.\n",
    "\n",
    "Expected output: 9\n",
    "\n",
    "Results: 11 - Because of our check D, we have 11 features with conflicting values in the dataset instead of the expected 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of issues: 11\n"
     ]
    }
   ],
   "source": [
    "# Because check A, B, C, and E are 0 therefor the total count of issues is check D\n",
    "if (checkA + checkB + checkC + checkE) == 0:\n",
    "    print(f\"Total count of issues: {checkD}\")\n",
    "else:\n",
    "    print(f\"This code needds to be updated to reflect the correct count of issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G - Identical Cases\n",
    "\n",
    "Refers to a situation where two or more cases contain identical values for all features including class label.\n",
    "\n",
    "Expected output: 2628\n",
    "\n",
    "Results: 1973 - There are 1973 identical cases in the dataset where the class label is also identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical cases in the dataset: 1973\n"
     ]
    }
   ],
   "source": [
    "checkG = len(df[df.duplicated()])   \n",
    "print(f\"Identical cases in the dataset: {checkG}\")\n",
    "\n",
    "cases_for_G = set()\n",
    "for index, row in df[df.duplicated()].iterrows():\n",
    "    cases_for_G.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H - Inconsistent cases\n",
    "\n",
    "As per G but the class labels differ, all other data item values are identical\n",
    "\n",
    "There are two identical modules M1 and M2 where M1 is labelled as fault free and M2 is labelled as faulty.\n",
    "\n",
    "Expected output: 889\n",
    "\n",
    "Results: 2061 - There are 2061 inconsistent cases in the dataset where the class label is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical feature values in the dataset: 2061\n"
     ]
    }
   ],
   "source": [
    "temp = df.copy()\n",
    "temp = temp.drop(columns=[target])\n",
    "checkH = len(temp[temp.duplicated()])\n",
    "print(f\"Identical feature values in the dataset: {checkH}\")\n",
    "\n",
    "cases_for_H = set()\n",
    "for index, row in temp[temp.duplicated()].iterrows():\n",
    "    cases_for_H.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Cases with missing values\n",
    "\n",
    "Counts the number of cases that contain one or more missing observations\n",
    "\n",
    "Expected output: 0\n",
    "\n",
    "Results: 0 - There are no cases with missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Get all the rows that have a missing value\n",
    "missing_rows = df[df.isnull().any(axis=1)]\n",
    "print(f\"Rows with missing values: {len(missing_rows)}\")\n",
    "\n",
    "cases_for_I = set()\n",
    "for index, row in missing_rows.iterrows():\n",
    "    cases_for_I.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J - Cases with conflicting feature values\n",
    "\n",
    "Counts cases that contain features (2 or more by definition) that violate some referential integrity constraint. Count each case irrespective of the number of features implicated\n",
    "\n",
    "As per Column D\n",
    "\n",
    "Expected output: 1287\n",
    "\n",
    "Results: 1262 - Like earlier with the features, the checks used in the document are not the same as the ones I have used. Instead of using the \"==\" operator to check for conflicting values, I have used the np.isclose() function to check for conflicting values. The tolerences were tweaked to get as close to the expected output as possible to give a close enough result of what the researchers might have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflicting cases count: 1262\n"
     ]
    }
   ],
   "source": [
    "conflicting_cases = set()\n",
    "for filter in conflicting_value_filters:\n",
    "    filter_df = df[~filter[\"filter\"]]\n",
    "    conflicting_cases.update(filter_df.index)\n",
    "\n",
    "checkI = len(conflicting_cases)\n",
    "\n",
    "print(f\"Conflicting cases count: {checkI}\")\n",
    "\n",
    "cases_for_J = set()\n",
    "for index in conflicting_cases:\n",
    "    cases_for_J.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - Cases with implausible values\n",
    "\n",
    "Counts cases that violate some integrity constraint. Count each case irrespective of the number of features implicated.\n",
    "\n",
    "As per Column E\n",
    "\n",
    "Expected output: 0\n",
    "\n",
    "Results: 0 - There are no cases with implausible values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implausible case count: 0\n"
     ]
    }
   ],
   "source": [
    "checkK = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for col in df.columns:\n",
    "        if is_implausible(row[col], col):\n",
    "            checkK += 1\n",
    "            print(f\"Row {index} has an implausible value {row[col]} for feature {col}\")\n",
    "\n",
    "print(f\"Implausible case count: {checkK}\")\n",
    "\n",
    "cases_for_K = set()\n",
    "for index, row in df.iterrows():\n",
    "    for col in df.columns:\n",
    "        if is_implausible(row[col], col):\n",
    "            cases_for_K.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L - Total of data quality problem cases\n",
    "\n",
    "Count of cases impacted by one or more of I to K that we denote DS0. Since cases may contain more than one problem this need not be the sum of I to K.\n",
    "\n",
    "Expected output: 1287\n",
    "\n",
    "Results: 1262 - Because of our check J, we have 1262 cases with conflicting feature values in the dataset instead of the expected 1287."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases for I to K: 1262\n"
     ]
    }
   ],
   "source": [
    "total_cases_I_to_K = set()\n",
    "total_cases_I_to_K.update(cases_for_I)\n",
    "total_cases_I_to_K.update(cases_for_J)\n",
    "total_cases_I_to_K.update(cases_for_K)\n",
    "\n",
    "print(f\"Total cases for I to K: {len(total_cases_I_to_K)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M - Total problem cases according to [6]\n",
    "\n",
    "Count of cases impacted by one or more of G to K denoted DS\n",
    "\n",
    "Expected output: 3158\n",
    "\n",
    "Results: 3228 - There are 3233 problem cases in the dataset according to the checks I have made. The checks I have are not the same as the ones in the document, but once again there are no way of knowing the methods used in the paper to check for conflicting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases for G to K: 3228\n"
     ]
    }
   ],
   "source": [
    "total_cases_G_to_K = set()\n",
    "total_cases_G_to_K.update(cases_for_G)\n",
    "total_cases_G_to_K.update(cases_for_H)\n",
    "total_cases_G_to_K.update(cases_for_I)\n",
    "total_cases_G_to_K.update(cases_for_J)\n",
    "total_cases_G_to_K.update(cases_for_K)\n",
    "\n",
    "print(f\"Total cases for G to K: {len(total_cases_G_to_K)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The followinng preprocessing algorithm was recreated from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, flag=False):\n",
    "    # Pull out the target feature\n",
    "    target_feature = ds.iloc[:, -1]\n",
    "    # Drop the target feature from the dataset\n",
    "    ds = ds.drop(columns=[target])\n",
    "\n",
    "    data = None\n",
    "    M = ds.shape[0]\n",
    "    N = ds.shape[1] \n",
    "    print(f\"Dataset shape: {M} x {N}\")\n",
    "\n",
    "    # step 1: remove cases with implausible values\n",
    "    implausible_cases = set()\n",
    "    for index, row in ds.iterrows():\n",
    "        for col in ds.columns:\n",
    "            if is_implausible(row[col], col):\n",
    "                implausible_cases.add(index)\n",
    "    data = ds.drop(index=implausible_cases)\n",
    "    \n",
    "    # step 2: remove cases with conflict feature values\n",
    "    data = ds.drop(index=conflicting_cases)\n",
    "\n",
    "    if flag:\n",
    "        # step 3: remove identical cases\n",
    "        data.drop_duplicates(inplace=True)\n",
    "\n",
    "        # step 4: remove inconsistent cases\n",
    "        data_without_target = data.copy()\n",
    "        duplicate_indices = data_without_target.duplicated()\n",
    "        data = data[~duplicate_indices]\n",
    "                \n",
    "    # step 5: remove cases with missing values\n",
    "    data = data.dropna()\n",
    "                \n",
    "    # step 6: remove constant features\n",
    "    data = data.loc[:, data.apply(pd.Series.nunique) != 1]\n",
    "                \n",
    "    # step 7: remove identical features\n",
    "    data = data.T.drop_duplicates().T\n",
    "\n",
    "    print(f\"Post processed dataset shape: {data.shape[0]} x {data.shape[1]}\")\n",
    "    print(f\"Removed {M - data.shape[0]} rows and {N - data.shape[1]} columns\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 10878 x 21\n",
      "Post processed dataset shape: 7650 x 21\n",
      "Removed 3228 rows and 0 columns\n",
      "Successfully saved preprocessed dataset to disk\n"
     ]
    }
   ],
   "source": [
    "df.loc[preprocess(df, True).index].to_pickle(ROOT+\"data/\"+DATASET.replace('csv','pkl'))\n",
    "print(\"Successfully saved preprocessed dataset to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
