{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Defects - 02 - Model\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from pprint import pprint \n",
    "\n",
    "DEBUG = True\n",
    "SEED = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"jm1.pkl\"\n",
    "\n",
    "import os, sys\n",
    "COLAB = 'google.colab' in sys.modules\n",
    "ROOT = \"./\"\n",
    "\n",
    "if COLAB:\n",
    "  from google.colab import drive\n",
    "  if not os.path.isdir(\"/content/gdrive\"):\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    d = \"/content/gdrive/MyDrive/datasets\"\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  ROOT = f\"/content/gdrive/MyDrive/datasets/{DATASET.replace(' ','_')}/\"\n",
    "  if not os.path.isdir(ROOT): os.makedirs(ROOT)\n",
    "\n",
    "\n",
    "def makedirs(d):\n",
    "  if COLAB:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d)\n",
    "  else:\n",
    "    if not os.path.isdir(ROOT+d): os.makedirs(ROOT+d, mode=0o777, exist_ok=True)\n",
    "\n",
    "for d in ['orig','data','output']: makedirs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7650, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC_BLANK</th>\n",
       "      <th>BRANCH_COUNT</th>\n",
       "      <th>LOC_CODE_AND_COMMENT</th>\n",
       "      <th>LOC_COMMENTS</th>\n",
       "      <th>CYCLOMATIC_COMPLEXITY</th>\n",
       "      <th>DESIGN_COMPLEXITY</th>\n",
       "      <th>ESSENTIAL_COMPLEXITY</th>\n",
       "      <th>LOC_EXECUTABLE</th>\n",
       "      <th>HALSTEAD_CONTENT</th>\n",
       "      <th>HALSTEAD_DIFFICULTY</th>\n",
       "      <th>HALSTEAD_EFFORT</th>\n",
       "      <th>HALSTEAD_ERROR_EST</th>\n",
       "      <th>HALSTEAD_LENGTH</th>\n",
       "      <th>HALSTEAD_LEVEL</th>\n",
       "      <th>HALSTEAD_PROG_TIME</th>\n",
       "      <th>HALSTEAD_VOLUME</th>\n",
       "      <th>NUM_OPERANDS</th>\n",
       "      <th>NUM_OPERATORS</th>\n",
       "      <th>NUM_UNIQUE_OPERANDS</th>\n",
       "      <th>NUM_UNIQUE_OPERATORS</th>\n",
       "      <th>LOC_TOTAL</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOC_BLANK  BRANCH_COUNT  LOC_CODE_AND_COMMENT  LOC_COMMENTS  CYCLOMATIC_COMPLEXITY  DESIGN_COMPLEXITY  ESSENTIAL_COMPLEXITY  LOC_EXECUTABLE  HALSTEAD_CONTENT  HALSTEAD_DIFFICULTY  HALSTEAD_EFFORT  HALSTEAD_ERROR_EST  HALSTEAD_LENGTH  HALSTEAD_LEVEL  HALSTEAD_PROG_TIME  HALSTEAD_VOLUME  NUM_OPERANDS  NUM_OPERATORS  NUM_UNIQUE_OPERANDS  NUM_UNIQUE_OPERATORS  LOC_TOTAL  label\n",
       "1        0.0         211.0                   0.0           0.0                  128.0              104.0                  14.0             0.0               0.0                  0.0              0.0                 0.0              0.0             0.0                 0.0              0.0           0.0            0.0                  0.0                   0.0     1129.0      1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(ROOT+\"data/\"+DATASET)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"label\"\n",
    "features = list(df.columns)\n",
    "features.remove(target)\n",
    "\n",
    "x = df[features]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "x = ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the models\n",
    "\n",
    "I've chosen the following models from earlier projects and I researched some other models that could be useful and added the last 2 namely `Multi-Layer Perceptron` and `Gaussian Naive Bayes` to the list of models to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='lbfgs', max_iter=10000, random_state=SEED),\n",
    "    \"Decision Tree (depth=0)\": DecisionTreeClassifier(random_state=SEED),\n",
    "    \"Decision Tree (depth=5)\": DecisionTreeClassifier(max_depth=5, random_state=SEED),\n",
    "    \"Decision Tree (depth=10)\": DecisionTreeClassifier(max_depth=10, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=SEED),\n",
    "    \"Ada Boost\": AdaBoostClassifier(algorithm=\"SAMME\", random_state=SEED),\n",
    "    \"Multi-Layer Perceptron\": MLPClassifier(random_state=SEED),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression trained in 0.03s\n",
      "Decision Tree (depth=0) trained in 0.19s\n",
      "Decision Tree (depth=5) trained in 0.05s\n",
      "Decision Tree (depth=10) trained in 0.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest trained in 3.02s\n",
      "Ada Boost trained in 0.65s\n",
      "Multi-Layer Perceptron trained in 5.37s\n",
      "Gaussian Naive Bayes trained in 0.00s\n"
     ]
    }
   ],
   "source": [
    "for name, model in classifiers.items():\n",
    "    t = time.time()\n",
    "    model.fit(x, y)\n",
    "    print(f\"{name} trained in {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the models\n",
    "\n",
    "The trained models will be evaluated using the following tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Logistic Regression`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `79.40%`\n",
       "\n",
       "Cross Validation (Test): `79.16% +/- 1.52%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5910  113]\n",
       " [1463  164]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.80      0.98      0.88      6023\n",
       "           1       0.59      0.10      0.17      1627\n",
       "\n",
       "    accuracy                           0.79      7650\n",
       "   macro avg       0.70      0.54      0.53      7650\n",
       "weighted avg       0.76      0.79      0.73      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Decision Tree (depth=0)`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `100.00%`\n",
       "\n",
       "Cross Validation (Test): `68.29% +/- 2.47%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[6023    0]\n",
       " [   0 1627]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       1.00      1.00      1.00      6023\n",
       "           1       1.00      1.00      1.00      1627\n",
       "\n",
       "    accuracy                           1.00      7650\n",
       "   macro avg       1.00      1.00      1.00      7650\n",
       "weighted avg       1.00      1.00      1.00      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Decision Tree (depth=5)`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `80.35%`\n",
       "\n",
       "Cross Validation (Test): `78.52% +/- 2.29%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5959   64]\n",
       " [1439  188]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.81      0.99      0.89      6023\n",
       "           1       0.75      0.12      0.20      1627\n",
       "\n",
       "    accuracy                           0.80      7650\n",
       "   macro avg       0.78      0.55      0.54      7650\n",
       "weighted avg       0.79      0.80      0.74      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Decision Tree (depth=10)`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `85.42%`\n",
       "\n",
       "Cross Validation (Test): `75.65% +/- 1.88%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5900  123]\n",
       " [ 992  635]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.86      0.98      0.91      6023\n",
       "           1       0.84      0.39      0.53      1627\n",
       "\n",
       "    accuracy                           0.85      7650\n",
       "   macro avg       0.85      0.68      0.72      7650\n",
       "weighted avg       0.85      0.85      0.83      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Random Forest`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `100.00%`\n",
       "\n",
       "Cross Validation (Test): `77.52% +/- 1.29%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[6023    0]\n",
       " [   0 1627]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       1.00      1.00      1.00      6023\n",
       "           1       1.00      1.00      1.00      1627\n",
       "\n",
       "    accuracy                           1.00      7650\n",
       "   macro avg       1.00      1.00      1.00      7650\n",
       "weighted avg       1.00      1.00      1.00      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Ada Boost`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `79.59%`\n",
       "\n",
       "Cross Validation (Test): `78.39% +/- 0.79%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5939   84]\n",
       " [1477  150]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.80      0.99      0.88      6023\n",
       "           1       0.64      0.09      0.16      1627\n",
       "\n",
       "    accuracy                           0.80      7650\n",
       "   macro avg       0.72      0.54      0.52      7650\n",
       "weighted avg       0.77      0.80      0.73      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Multi-Layer Perceptron`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `81.29%`\n",
       "\n",
       "Cross Validation (Test): `78.55% +/- 1.54%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5945   78]\n",
       " [1353  274]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.81      0.99      0.89      6023\n",
       "           1       0.78      0.17      0.28      1627\n",
       "\n",
       "    accuracy                           0.81      7650\n",
       "   macro avg       0.80      0.58      0.58      7650\n",
       "weighted avg       0.81      0.81      0.76      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Evaluation Report `Gaussian Naive Bayes`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Accuracy (Train): `78.00%`\n",
       "\n",
       "Cross Validation (Test): `77.84% +/- 2.12%`\n",
       "\n",
       "Confusion Matrix:\n",
       "`[[5613  410]\n",
       " [1273  354]]`\n",
       "\n",
       "Classification Report:\n",
       "```              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.82      0.93      0.87      6023\n",
       "           1       0.46      0.22      0.30      1627\n",
       "\n",
       "    accuracy                           0.78      7650\n",
       "   macro avg       0.64      0.57      0.58      7650\n",
       "weighted avg       0.74      0.78      0.75      7650\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best = None\n",
    "totals = {}\n",
    "\n",
    "for name, model in classifiers.items():\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    accuracy_train = accuracy_score(y, y_pred)\n",
    "\n",
    "    scores = cross_val_score(model, x, y, cv=10, n_jobs=-1)\n",
    "\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "\n",
    "    classification = classification_report(y, y_pred)\n",
    "\n",
    "    # Add a markdown cell to the notebook\n",
    "    display(Markdown(f\"## Evaluation Report `{name}`\"))\n",
    "    display(Markdown(f\"Accuracy (Train): `{accuracy_train:.2%}`\\n\\nCross Validation (Test): `{np.mean(scores):.2%} +/- {np.std(scores):.2%}`\\n\\nConfusion Matrix:\\n`{confusion}`\\n\\nClassification Report:\\n```{str(classification)}```\"))\n",
    "    display(Markdown(f\"---\"))\n",
    "\n",
    "    if best is None or np.mean(scores) > best[1]:\n",
    "        best = (name, np.mean(scores))\n",
    "\n",
    "    totals[name] = np.mean(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on the models:\n",
    "\n",
    "* Overfitting: Classifiers, `Decision Tree`, `Random Forest`, and `Multi-Layer Perceptron` shows signs of overfitting. The training accuracy is significantly higher than the validation accuracy. MLP only looks like a slight overfitting. \n",
    "* Underfitting: There are no signs of underfitting.\n",
    "* Class performance: All the models have better performance when it comes to predicting `class 0`, which is expected as the dataset is imbalanced and favors `class 0`.\n",
    "* Based on cross-validation, the best model is `Logistic Regression` with a mean accuracy of `79.16%`, but I dont think it is the best model because of the imbalance in the precision between predicting `class 0` and `class 1`.\n",
    "* I feel that the best model is between `Multi-Layer Perceptron`, `Decision Tree (Depth=5)`, and `Decision Tree (Depth=10)`.\n",
    "    * `Multi-Layer Perceptron` has fairly well balanced precision between `class 0` and `class 1`, but it is slightly overfitting. It has a good mean accuracy of `78.55%.`\n",
    "    * `Decision Tree (Depth=5)` has the same good mean accuracy of `78.52%` and it is not overfitting. It has a slightly better precision for `class 1`, but still well balanced.\n",
    "    * `Decision Tree (Depth=10)` has the best balance between precision for `class 0` and `class 1`, but it is overfitting hugely. \n",
    "* Based on the observations above I will choose `Decision Tree (Depth=5)` as the best model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
